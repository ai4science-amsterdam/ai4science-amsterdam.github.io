I"=<blockquote>
  <p>Knowledge Shared = Knowledge<sup>2</sup></p>
</blockquote>

<p>The AI4Science Colloquium is a bi-weekly colloquium series, held on alternating Tuesdays at 14:00 Central European Time. In this colloquium our very own Teodora Pandeva and Fiona Lippert invite renowned speakers to present and discuss their state-of-the-art AI solutions for scientific discovery. Interested? Subscribe to our Email-list to be notified.</p>

<h2 id="email-list">Email List</h2>
<p>To stay up to date with our activities and be invited to our biweekly AI4Science colloquium series, you may send a request to be included in our emaillist via <a href="/contact/">an email to us</a> with your name, affiliation and a one-sentence motivation for joining.</p>

<h2 id="next-colloquium">Next Colloquium</h2>

<h4 id="conformal-prediction-under-distribution-shift">Conformal prediction under distribution shift</h4>

<p>Date: 14-03-2023 14:00-1500 Central European Winter time</p>

<p><img src="../people/AadityaRamdas.png" alt="Aaditya Ramdas" width="100" style="float: right; margin-right: 10px; border-radius:50%;" /></p>

<p>Speaker: <strong>Aaditya Ramdas</strong>, Carnegie Mellon University</p>

<p><strong>Abstract:</strong> <br /></p>

<p>Conformal prediction is a modern technique for quantifying predictive uncertainty for arbitrary ML models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice, and in such settings, we might want to use an algorithm that treats observations asymmetrically (eg: upweighting more recent observations).This paper proposes a new methodology to deal with both aspects: we use weighted quantiles to introduce robustness against distribution drift, and design a new technique to allow for asymmetric algorithms. Our algorithms are provably robust, with substantially less loss of coverage under distribution drift or shift, while also reducing to the same algorithm and coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable.This is joint work with Rina Barber, Emmanuel Candes and Ryan Tibshirani. A preprint is at <a href="https://arxiv.org/abs/2202.13415">https://arxiv.org/abs/2202.13415</a>.</p>

<p><strong>Bio:</strong> <br /></p>

<p>Aaditya Ramdas (PhD, 2015) is an assistant professor at Carnegie Mellon University, in the Departments of Statistics and Machine Learning. He was a postdoc at UC Berkeley (2015â€“2018) and obtained his PhD at CMU (2010â€“2015), receiving the Umesh K. Gavaskar Memorial Thesis Award. Aaditya was an inaugural winner of the COPSS Leadership Award, and a recipient of the 2021 Bernoulli New Researcher Award. His work is supported by an NSF CAREER Award, an Adobe Faculty Research Award (2020), a Google Research Scholar award (2022), amongst others. He was a CUSO lecturer in 2022, and will be a Lunteren lecturer in 2023. Aadityaâ€™s main theoretical and methodological research interests include post-selection inference (interactive, structured, online, post-hoc control of false decision rates, etc), game-theoretic statistics (sequential uncertainty quantification, confidence sequences, always-valid p-values, safe anytime-valid inference, e-processes, supermartingales, e-values, etc), and distribution-free black-box predictive inference (conformal prediction, calibration, etc). His areas of applied interest include privacy, neuroscience, genetics and auditing, and his groupâ€™s work has received multiple best paper awards.</p>

<h2 id="schedule">Schedule</h2>

<ul>
  <li>18 January 2022 - <strong>Andrew Ferguson</strong></li>
  <li>7 February 2022 - <strong>Jan-Matthis LÃ¼ckmann</strong></li>
  <li>1 March 2022 - <strong>Martin van Hecke</strong></li>
  <li>15 March 2022 - <strong>Rajesh Ranganath</strong></li>
  <li>29 March 2022 - <strong>Anna Scaife</strong></li>
  <li>12 April 2022 - <strong>Gabriel VivÃ³-Truyols</strong></li>
  <li>26 April 2022 - <strong>Maximilian Dax</strong></li>
  <li>24 May 2022 - <strong>Francesca Grisoni</strong></li>
  <li>7 June 2022 - <strong>Wujie Wang</strong></li>
  <li>21 June 2022 - <strong>Peter GrÃ¼nwald</strong></li>
  <li>5 July 2022 - <strong>Michele Ceriotti</strong></li>
  <li>11 October 2022 - <strong>Guy Wolf</strong></li>
  <li>25 October 2022 - <strong>Fredrik Lindsten</strong></li>
  <li>14 March 2023 - <strong>Aaditya Ramdas</strong></li>
</ul>

<h2 id="previous-colloquium">Previous Colloquium</h2>

<h4 id="deep-gaussian-markov-random-fields">Deep Gaussian Markov Random Fields</h4>

<p>Date: 25-10-2022 14:00-1500 Central European Summer time</p>

<p><img src="../people/FredrikLindsten.png" alt="FredrikLindsten" width="100" style="float: right; margin-right: 10px; border-radius:50%;" /></p>

<p>Speaker: <strong>Fredrik Lindsten</strong>, LinkÃ¶ping University</p>

<p><strong>Abstract:</strong> <br /></p>

<p>Machine learning methods on graphs are relevant for many application domains due to their ability to model complex dependencies and structures. Gaussian Markov Random Fields (GMRFs) provide a principled way to define Gaussian models on graphs by utilizing their sparsity structure. In this talk I will show how we can use graph neural networks (GNNs) and convolutional neural networks (CNNs) do design scalable and flexible GMRFs. Starting with lattice graphs, we establish a formal connection between CNNs and GMRFs, by showing that common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. I will also discuss how this Deep GMRF can be generalized to arbitrary graphs using a specialized GNN layer. Well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the Deep GMRF, and for a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. I demonstrate the flexibility of the proposed model and show that it compares favorably to other methods, both Bayesian and deep-learning-based, on spatial and non-spatial data.</p>

<p>Joint work with Joel Oskarsson (LiU) and Per SidÃ©n (LiU/Qualcomm Arriver)</p>

<p>Papers:</p>

<p><a href="https://arxiv.org/abs/2002.07467">Deep Gaussian Markov Random Fields</a></p>

<p><a href="https://arxiv.org/abs/2206.05032">Scalable Deep Gaussian Markov Random Fields for General Graphs</a></p>

<!--
<a class="radius button small" href="https://drive.google.com/file/d/1piVbnetRwbMxMFyVIgoq1cOvAw3BlqBP/view?usp=sharing">Watch Back â€º</a>
-->

<p>For an overview of more  previous colloquia, please have a look at out <a href="/blog/">blog</a>.</p>

:ET